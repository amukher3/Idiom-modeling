{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an effort to model english language idiom using Topic modeling. The topic modeling approach used here is the Latent\n",
    "Dirchelet's allocation(LDA). One of the potential applications of this effort is to get the appropriate idiom from its corresponding meaning or even from \"bits and pieces\" of its english language meaning. \n",
    "\n",
    "With the current database, its usage is pretty limited but with a increase in the size of the database and with a better `similarity metric` this method of scrapping off the idiom from its english language meaning or even from the parts of its meaning might turn out to be more than handy. \n",
    "\n",
    "Other methods of modeling the topic such as `matrix factorization` can also be used to check its effectiveness in comparison to `LDA`. \n",
    "\n",
    "This is still work in progress. I intend to add some vizualisations to show the clusters of the topics also may be try out with other topic models and make it more dependent on topic modeling approach. Improving the user interface after increasing the data-base might not be such a bad idea afterall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Created on Sat May  2 10:57:30 2020\n",
    "\n",
    "@author:Abhishek Mukherjee\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_beside=pd.read_csv(\"C:/Users/abhi0/OneDrive/Documents/Idioms_meaning_collection/IdiomBeside-2.csv\")\n",
    "dfIdioms=list(df_beside['A hot potato'])\n",
    "notNullIdx_1=np.where(df_beside[df_beside.columns[1]].isnull()==False)\n",
    "\n",
    "for i in (notNullIdx_1[0][:]):\n",
    "    print(i)\n",
    "\n",
    "df_meanings=['']*len(dfIdioms)\n",
    "\n",
    "temp=notNullIdx_1[0][:]\n",
    "\n",
    "df_meanings[temp[0]]=df_beside[df_beside.columns[1]][notNullIdx_1[0][0]]\n",
    "\n",
    "numCol=5 #number of columns not taking the first into consideration\n",
    "notNullIdx=[]\n",
    "\n",
    "for i in range(1,6):\n",
    "    notNullIdx.append(np.where(df_beside[df_beside.columns[i]].isnull()==False))\n",
    "    temp=notNullIdx[i-1]\n",
    "    for k in range(len(temp[0])):\n",
    "        df_meanings[temp[0][k]]=df_beside[df_beside.columns[i]][temp[0][k]]  \n",
    "        \n",
    "meanFrame=[]\n",
    "IdiomFrame=[]\n",
    "meanFrame=pd.DataFrame(df_meanings)        \n",
    "IdiomFrame=pd.DataFrame(dfIdioms)\n",
    "\n",
    "result_1=pd.concat([IdiomFrame,meanFrame],axis=1)\n",
    "###############################################################################\n",
    "        \n",
    "#For the other csv files: \n",
    "df_below=\\\n",
    "pd.read_csv(\"C:/Users/abhi0/OneDrive/Documents/Idioms_meaning_collection/IdiomBelow_1.csv\")   \n",
    "\n",
    "meanList=[]\n",
    "idiomList=[]\n",
    "for i in range(1,len(df_below),2):\n",
    "    temp=df_below['Idiom'][i]\n",
    "    startIdx=temp.find(' ')\n",
    "    meanList.append(temp[startIdx+1:])\n",
    "    idiomList.append(df_below['Idiom'][i-1])\n",
    " \n",
    "meanFrame=pd.DataFrame(meanList)\n",
    "IdiomFrame=pd.DataFrame(idiomList)\n",
    "\n",
    "result_2=pd.concat([IdiomFrame,meanFrame],axis=1)\n",
    "\n",
    "############################################################################$\n",
    "#For the otehr below csv file: \n",
    "\n",
    "df_below2=\\\n",
    "pd.read_csv(\"C:/Users/abhi0/OneDrive/Documents/Idioms_meaning_collection/IdiomBelow_2.csv\")       \n",
    "         \n",
    "tempList=list(df_below2[df_below2.columns[0]][1:])\n",
    "\n",
    "meanList=[]\n",
    "idiomList=[]\n",
    "\n",
    "for i in range(1,len(tempList),2):\n",
    "    temp=tempList[i-1]\n",
    "    meanList.append(temp)\n",
    "    idiomList.append(tempList[i])\n",
    "    \n",
    "meanFrame=pd.DataFrame(meanList)\n",
    "IdiomFrame=pd.DataFrame(idiomList)\n",
    "\n",
    "result_3=pd.concat([IdiomFrame,meanFrame],axis=1)    \n",
    "    \n",
    "############################################################################## \n",
    "###  \n",
    "df_beside=\\\n",
    "pd.read_csv(\"C:/Users/abhi0/OneDrive/Documents/Idioms_meaning_collection/IdiomBeside.csv\")\n",
    " \n",
    "meanList=[]\n",
    "idiomList=[]\n",
    "\n",
    "idiomList=list(df_beside[df_beside.columns[0]])\n",
    "meanList=list(df_beside[df_beside.columns[1]])   \n",
    "\n",
    "meanFrame=pd.DataFrame(meanList)\n",
    "IdiomFrame=pd.DataFrame(idiomList) \n",
    "\n",
    "result_4=pd.concat([IdiomFrame,meanFrame],axis=1)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "df3=\\\n",
    "pd.read_csv(\"C:/Users/abhi0/OneDrive/Documents/Idioms_meaning_collection/IdiomBelow_3.csv\")\n",
    "temp=[]  \n",
    "idiomList=[]\n",
    "meanList=[] \n",
    "\n",
    "import re\n",
    " \n",
    "for i in range(len(df3)):\n",
    "    tempPrime=df3['Idioms and meaning'][i] \n",
    "    if '–' not in tempPrime:\n",
    "        continue\n",
    "    else:\n",
    "        temp=re.split('\\s\\–\\s',tempPrime)\n",
    "        idiomList.append(temp[0])\n",
    "        meanList.append(temp[1])\n",
    "        \n",
    "meanFrame=pd.DataFrame(meanList)\n",
    "IdiomFrame=pd.DataFrame(idiomList) \n",
    "\n",
    "result_5=pd.concat([IdiomFrame,meanFrame],axis=1)  \n",
    "\n",
    "##############################################################################\n",
    "df4=\\\n",
    "pd.read_csv(\"C:/Users/abhi0/OneDrive/Documents/Idioms_meaning_collection/Idiom_Below4.csv\")\n",
    "\n",
    "temp=[]\n",
    "idiomList=[]\n",
    "meanList=[]\n",
    "\n",
    "for i in range(len(df4)):\n",
    "    \n",
    "    tempPrime=df4[df4.columns[0]][i]\n",
    "    \n",
    "    if ':' not in tempPrime:\n",
    "        continue\n",
    "    else:\n",
    "        temp=df4[df4.columns[0]][i].split(':')\n",
    "        idiomList.append(temp[0])\n",
    "        meanList.append(temp[1])\n",
    "        \n",
    "meanFrame=pd.DataFrame(meanList)\n",
    "IdiomFrame=pd.DataFrame(idiomList) \n",
    "\n",
    "result_6=pd.concat([IdiomFrame,meanFrame],axis=1)  \n",
    "\n",
    "result_6.isnull().sum()\n",
    "result_6.isna().sum()\n",
    "        \n",
    "########################### #Merging the data-frames: ########################\n",
    "\n",
    "finalResult=pd.concat([result_1,result_2,\\\n",
    "                       result_3,result_4,result_5,result_6],axis=0,ignore_index=True)\n",
    "    \n",
    "#Adding header:\n",
    "finalResult.columns=['Idiom','Meaning']  \n",
    "\n",
    "finalResult.dropna(inplace=True)  \n",
    "finalResult.reset_index(drop=True,inplace=True)    \n",
    "\n",
    "##looking for special characters in a the meaning\n",
    "##might contain another meaning,can be moved to another list\n",
    "#looking for headings\n",
    "string_check= re.compile('[@_#$%^&()<>/\\|;}{~:0-9]')\n",
    "tempIdx=[]\n",
    "tempStr=[]\n",
    "    \n",
    "for i in range(len(finalResult['Meaning'])):\n",
    "    \n",
    "    if(len(finalResult['Meaning'][i])==0):\n",
    "        continue\n",
    "    else:\n",
    "        if(string_check.search(finalResult['Meaning'][i]) == None):\n",
    "            continue\n",
    "        else: \n",
    "            tempStr.append(finalResult['Meaning'][i])\n",
    "            tempIdx.append(i)\n",
    "            if finalResult['Meaning'][i].isupper()==True:\n",
    "                finalResult.drop(i,inplace=True)  \n",
    "                \n",
    "   \n",
    "    \n",
    "#Removing some unnecessary rows: \n",
    "Idx=[]            \n",
    "Idx=np.where(finalResult['Meaning']=='Meaning')     \n",
    "for i in range(len(Idx)):\n",
    "    finalResult.drop(Idx[i],inplace=True)   \n",
    "\n",
    "##Move the double-meanings of the idioms or duplicate\n",
    "\n",
    "\n",
    "    \n",
    "      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idiom</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A penny for your thoughts</td>\n",
       "      <td>A way of asking what someone is thinking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Actions speak louder than words</td>\n",
       "      <td>People's intentions can be judged better by wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Add insult to injury</td>\n",
       "      <td>To further a loss with mockery or indignity; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>At the drop of a hat</td>\n",
       "      <td>without any hesitation; instantly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Back to the drawing board</td>\n",
       "      <td>When an attempt fails and it's time to start a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1806</td>\n",
       "      <td>You’re Driving Me Nuts</td>\n",
       "      <td>To make someone giddy or crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1807</td>\n",
       "      <td>Yours Truly</td>\n",
       "      <td>Me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>Zero In On</td>\n",
       "      <td>Focus closely on something; take aim at somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>Zig When One Should Be Zagging</td>\n",
       "      <td>To make an error; to choose an incorrect course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>Zip One’s Lip</td>\n",
       "      <td>Be quiet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1809 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Idiom  \\\n",
       "0           A penny for your thoughts   \n",
       "1     Actions speak louder than words   \n",
       "2                Add insult to injury   \n",
       "3                At the drop of a hat   \n",
       "4           Back to the drawing board   \n",
       "...                               ...   \n",
       "1806           You’re Driving Me Nuts   \n",
       "1807                      Yours Truly   \n",
       "1808                       Zero In On   \n",
       "1809   Zig When One Should Be Zagging   \n",
       "1810                    Zip One’s Lip   \n",
       "\n",
       "                                                Meaning  \n",
       "0              A way of asking what someone is thinking  \n",
       "1     People's intentions can be judged better by wh...  \n",
       "2     To further a loss with mockery or indignity; t...  \n",
       "3                    without any hesitation; instantly.  \n",
       "4     When an attempt fails and it's time to start a...  \n",
       "...                                                 ...  \n",
       "1806                     To make someone giddy or crazy  \n",
       "1807                                                 Me  \n",
       "1808   Focus closely on something; take aim at somet...  \n",
       "1809    To make an error; to choose an incorrect course  \n",
       "1810                                           Be quiet  \n",
       "\n",
       "[1809 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idiom</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A penny for your thoughts</td>\n",
       "      <td>A way of asking what someone is thinking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Actions speak louder than words</td>\n",
       "      <td>People's intentions can be judged better by wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Add insult to injury</td>\n",
       "      <td>To further a loss with mockery or indignity; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>At the drop of a hat</td>\n",
       "      <td>without any hesitation; instantly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Back to the drawing board</td>\n",
       "      <td>When an attempt fails and it's time to start a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Ball is in your court</td>\n",
       "      <td>It is up to you to make the next decision or step</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Idiom  \\\n",
       "0        A penny for your thoughts   \n",
       "1  Actions speak louder than words   \n",
       "2             Add insult to injury   \n",
       "3             At the drop of a hat   \n",
       "4        Back to the drawing board   \n",
       "5            Ball is in your court   \n",
       "\n",
       "                                             Meaning  \n",
       "0           A way of asking what someone is thinking  \n",
       "1  People's intentions can be judged better by wh...  \n",
       "2  To further a loss with mockery or indignity; t...  \n",
       "3                 without any hesitation; instantly.  \n",
       "4  When an attempt fails and it's time to start a...  \n",
       "5  It is up to you to make the next decision or step  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalResult.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e6f4685d8ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinalResult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Meaning'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import re\n",
    "re.findall(';',finalResult['Meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1809\n",
      "1970\n"
     ]
    }
   ],
   "source": [
    "idxList=[]\n",
    "temp=''\n",
    "\n",
    "meanFrame=pd.DataFrame(finalResult['Meaning'])\n",
    "\n",
    "c=0\n",
    "tempMeaning=list(finalResult['Meaning'])\n",
    "tempIdioms=list(finalResult['Idiom'])\n",
    "tempList=[]\n",
    "tempPrime=[]\n",
    "\n",
    "print(len(tempMeaning))\n",
    "\n",
    "for i in range(len(tempMeaning)):\n",
    "    if re.search('\\;+',tempMeaning[i]):\n",
    "        tempList=re.split('\\;+',tempMeaning[i])\n",
    "        tempPrime.append(tempList)\n",
    "        tempMeaning.pop(i)\n",
    "        for j in range(len(tempList)):\n",
    "            tempIdioms.append(tempIdioms[i])\n",
    "            tempMeaning.append(tempList[j])\n",
    "        \n",
    "        idxList.append(i)\n",
    "        tempIdioms.pop(i)\n",
    "        tempList=[]\n",
    "\n",
    "        \n",
    "print(len(tempMeaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging columsn into a dataframe:\n",
    "df = pd.DataFrame(\n",
    "    {'Meaning': tempMeaning,\n",
    "     'Idioms': tempIdioms,\n",
    "    })      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Idioms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A way of asking what someone is thinking</td>\n",
       "      <td>A penny for your thoughts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>People's intentions can be judged better by wh...</td>\n",
       "      <td>Actions speak louder than words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>without any hesitation; instantly.</td>\n",
       "      <td>At the drop of a hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>When an attempt fails and it's time to start a...</td>\n",
       "      <td>Back to the drawing board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>It is up to you to make the next decision or step</td>\n",
       "      <td>Ball is in your court</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Meaning  \\\n",
       "0           A way of asking what someone is thinking   \n",
       "1  People's intentions can be judged better by wh...   \n",
       "2                 without any hesitation; instantly.   \n",
       "3  When an attempt fails and it's time to start a...   \n",
       "4  It is up to you to make the next decision or step   \n",
       "\n",
       "                            Idioms  \n",
       "0        A penny for your thoughts  \n",
       "1  Actions speak louder than words  \n",
       "2             At the drop of a hat  \n",
       "3        Back to the drawing board  \n",
       "4            Ball is in your court  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(df['Meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1970x1071 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4929 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=7, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accordance\n",
      "hold\n",
      "marked\n",
      "news\n",
      "bar\n",
      "manner\n",
      "encounter\n",
      "particularly\n",
      "events\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,len(cv.get_feature_names()))\n",
    "    print(cv.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming\n",
      "healthy\n",
      "following\n",
      "written\n",
      "confident\n",
      "prepared\n",
      "annoyance\n",
      "admit\n",
      "performance\n",
      "useless\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,len(cv.get_feature_names()))\n",
    "    print(cv.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14276422, 0.14294866, 0.14285758, ..., 1.14278607, 3.14421721,\n",
       "        1.14280681],\n",
       "       [0.14285738, 0.14285745, 1.14280186, ..., 0.14285746, 0.14285769,\n",
       "        0.1428577 ],\n",
       "       [0.14285741, 0.1433159 , 1.14291041, ..., 0.14285749, 1.14249687,\n",
       "        0.14285776],\n",
       "       ...,\n",
       "       [0.14285739, 0.14285746, 0.14285756, ..., 0.14285746, 0.1428577 ,\n",
       "        1.14223074],\n",
       "       [0.14285734, 0.1428574 , 0.14285748, ..., 0.1428574 , 0.14285759,\n",
       "        0.1428576 ],\n",
       "       [0.14285734, 4.14230562, 0.14285749, ..., 1.14292661, 2.1417015 ,\n",
       "        2.14316169]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA.components_)\n",
    "LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['answer', 'lose', 'effect', 'let', 'state', 'tired', 'strong', 'badly', 'taking', 'miss', 'music', 'advance', 'leave', 'cause', 'problems', 'accept', 'work', 'mistake', 'future', 'isn', 'temper', 'obvious', 'lot', 'better', 'problem', 'place', 'unimportant', 'ordinary', 'disagreement', 'dead', 'absolutely', 'words', 'little', 'stop', 'set', 'energy', 'attention', 'completely', 'say', 'risk', 'large', 'opportunity', 'follow', 'success', 'having', 'people', 'deal', 'great', 'don', 'talk']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['tendency', 'finished', 'accomplish', 'slow', 'opportunities', 'unexpected', 'law', 'partner', 'expensive', 'change', 'choices', 'dangerous', 'women', 'job', 'ideas', 'relationship', 'reveal', 'forced', 'subject', 'does', 'romantic', 'best', 'anger', 'begin', 'organization', 'things', 'express', 'impossible', 'delay', 'responsibilities', 'family', 'control', 'difficult', 'ready', 'common', 'strong', 'healthy', 'understand', 'point', 'similar', 'task', 'group', 'different', 'way', 'secret', 'position', 'feel', 'quickly', 'doing', 'people']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['hurt', 'spectacular', 'condition', 'conflict', 'making', 'number', 'familiar', 'game', 'phrase', 've', 'perspective', 'standard', 'minutes', 'job', 'upset', 'deeply', 'usually', 'process', 'worth', 'end', 'lie', 'annoyed', 'forget', 'inexperienced', 'particular', 'occur', 'pay', 'basic', 'unable', 'quality', 'away', 'highly', 'treated', 'wait', 'coming', 'happen', 'likely', 'used', 'field', 'idea', 'new', 'moment', 'certain', 'past', 'way', 'angry', 'trouble', 'especially', 'extremely', 'person']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['ll', 'calm', 'general', 'prepare', 'using', 'lose', 'especially', 'period', 'given', 'thing', 'short', 'having', 'different', 'did', 'item', 'despite', 'patience', 'fight', 'combative', 'area', 'course', 'day', 'intentionally', 'makes', 'busy', 'hard', 'cost', 'distance', 'argue', 'eager', 'contest', 'use', 'plan', 'prepared', 'expected', 'working', 'action', 'problems', 'high', 'competition', 'does', 'want', 'conflict', 'usually', 'physically', 'influence', 'small', 'said', 'work', 'person']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['place', 'possible', 'usually', 'resources', 'totally', 'sale', 'alcoholic', 'details', 'thinking', 'age', 'said', 'close', 'death', 'error', 'work', 'happening', 'familiar', 'temporary', 'admit', 'progress', 'believe', 'experience', 'strongly', 'tell', 'need', 'drink', 'attack', 'defeat', 'know', 'run', 'severely', 'attempt', 'avoid', 'help', 'better', 'agree', 'problem', 'reveal', 'plans', 'small', 'making', 'attractive', 'late', 'difficult', 'used', 'fast', 'oneself', 'opinion', 'don', 'accept']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['sleep', 'save', 'medicine', 'improve', 'end', 'home', 'risks', 'start', 'relationship', 'solve', 'die', 'far', 'test', 'used', 'right', 'decline', 'value', 'solution', 'succeed', 'contact', 'appearance', 'chance', 'times', 'task', 'situation', 'argument', 'job', 'luck', 'force', 'work', 'person', 'thing', 'health', 'later', 'going', 'decision', 'wrong', 'hard', 'order', 'worse', 'easy', 'important', 'long', 'problem', 'money', 'bad', 'things', 'good', 'make', 'time']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['ahead', 'true', 'pleasant', 'trouble', 'actions', 'similar', 'sick', 'accomplishment', 'ill', 'means', 'especially', 'say', 'generally', 'typically', 'said', 'story', 'use', 'happy', 'slightly', 'look', 'talking', 'somebody', 'unpleasant', 'love', 'hear', 'bad', 'positive', 'group', 'think', 'issue', 'possible', 'event', 'avoid', 'usually', 'feeling', 'used', 'new', 'crazy', 'just', 'blame', 'people', 'problem', 'person', 'advantage', 'act', 'try', 'negative', 'difficult', 'way', 'situation']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-50:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "enqSim=\"being betrayed\"\n",
    "doc1=[enqSim]\n",
    "vect2=count_vect.fit_transform(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    " tempVect2=np.vectorize(enqSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 2747)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1970"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer1.shape)\n",
    "len(df['Meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "temp= count_vect.fit_transform(df['Meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 2747)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "tempPrime=count_vect.fit_transform(doc1)\n",
    "tempPrime.shape\n",
    "print(temp.shape)\n",
    "print(tempPrime.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(temp[:,1],tempPrime[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "simMat=cosine_similarity(temp[:,1],tempPrime[:,1])\n",
    "df['Meaning'].shape\n",
    "\n",
    "idx=[]\n",
    "\n",
    "for i in simMat:\n",
    "    if i!=0:\n",
    "        idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])]\n"
     ]
    }
   ],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine=[]\n",
    "\n",
    "#Enter the meaning or the\n",
    "#kind of sentence you want \n",
    "#to look-up the idiom for.\n",
    "text1 = \"a way of asking\"\n",
    "\n",
    "\n",
    "for i in df['Meaning']:\n",
    "    text1 = \"a way of asking\"\n",
    "    text2 = i\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "    cosine.append(get_cosine(vector1, vector2))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2773500981126146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3779644730092272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "temp=np.argmax(cosine)\n",
    "print(temp)\n",
    "#temp.index(np.argmax(cosine))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close match Idiom is:Raise the Roof and the corresponding meaning is:  Make a great deal of noise (said of a crowd)\n",
      "Close match Idiom is:A penny for your thoughts and the corresponding meaning is: A way of asking what someone is thinking\n",
      "Close match Idiom is:Set something to Music and the corresponding meaning is:  To write a piece of music to accompany a set of words\n",
      "Close match Idiom is:Set something to Music and the corresponding meaning is:  To write a piece of music to accompany a set of words\n"
     ]
    }
   ],
   "source": [
    "Idx=[]\n",
    "c=0\n",
    "temp=np.sort(cosine)\n",
    "for i in range(1,5):\n",
    "    Idx=cosine.index(temp[len(temp)-i])\n",
    "    print(f\"Close match Idiom is:{df['Idioms'][Idx]} and the corresponding meaning is: {df['Meaning'][Idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                A way of asking what someone is thinking\n",
       "1       People's intentions can be judged better by wh...\n",
       "2                      without any hesitation; instantly.\n",
       "3       When an attempt fails and it's time to start a...\n",
       "4       It is up to you to make the next decision or step\n",
       "                              ...                        \n",
       "1965     to talk about something of which one has litt...\n",
       "1966                                         I don’t know\n",
       "1967                                       I have no idea\n",
       "1968                           Focus closely on something\n",
       "1969                                take aim at something\n",
       "Name: Meaning, Length: 1970, dtype: object"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Meaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<range_iterator at 0x19bd51bb090>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed(range(len(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
